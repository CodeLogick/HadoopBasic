Default replication factor for Hadoo is 3


Hadoop Distributions 
1. Cloudra
2. Hortonworks

For this training we are using Cloudra.

Defining lambda in Python : lamda s : s.length()

## HDFS Commands ::
1. Create a folder : hdfs dfs -mkdir /hadooptraining
2. Put (to upload a file to hdfs system) cmd : hdfs dfs -put first.txt /hadooptraining
3. To see the content of file : hdfs dfs -cat /hadooptraining/first.txt
4. Copying file from one folder to another : hdfs dfs -cp /hadooptraining/first.txt /hadooptraining1
5. To see file content : hdfs dfs -cat /MyHadoop/orders/part-m-00000


## To list java processes alone :: sudo jps
secondary name node does housekeeping job for main name node.

## Downloading a file from hdfs to local client : 
hdfs dfs -get /hadooptraining1/first.txt

## GUI for Cloudera Hadoop is HUE
default username and pass is cloudera/cloudera




DB Table can be uploaded in 3 formats to hdfs:
1. Default - CSV
2. Parquet - Columnar
3. Avro - Binay

# To updload a table in to HDFS
By default it will upload in csv format:
sqoop import --connect jdbc:mysql://localhost:3306/retail_db --username root --password cloudera --table orders --target-dir /MyHadoop
sqoop import --connect jdbc:mysql://localhost:3306/hadooptrainingdb --username root --password cloudera --table employee --target-dir /MyHadoop/employee -m 1


# upload table in parquet format
sqoop import --connect jdbc:mysql://localhost:3306/hadooptrainingdb --username root --password cloudera --table employee --target-dir /MyHadoop/employeeparquet -m 1 --as-parquetfile
-m is used for limiting the threads.

# upload table in parquet format (by selecting few columns)
sqoop import --connect jdbc:mysql://localhost:3306/hadooptrainingdb --username root --password cloudera --table employee --target-dir /MyHadoop/employeeparquetselect -m 1 --as-parquetfile --columns emp_id where emp_id = 101

# upload table in avro format
sqoop import --connect jdbc:mysql://localhost:3306/hadooptrainingdb --username root --password cloudera --table employee --target-dir /MyHadoop/employeeavro -m 1 --as-avrodatafile


To view in termina there is parquet cmd:
parquet-tools cat hdfs://localhost/MyHadoop/employeeparquet/ebaf37fa-c08c-4563-9817-e42f21d52ad7.parquet

To view as in DB
parquet-tools schema hdfs://localhost/MyHadoop/employeeparquet/ebaf37fa-c08c-4563-9817-e42f21d52ad7.parquet

To view in json
parquet-tools cat --json hdfs://localhost/MyHadoop/employeeparquet/ebaf37fa-c08c-4563-9817-e42f21d52ad7.parquet

To view avro in json
avro-tools tojson hdfs://localhost/MyHadoop/employeeavro/part-m-00000.avro




##### Incremental data upload to HDFS from DB

Two Modes : 
1. Merge Mode : updates the existing file
2. Append Mode : New file will be ceated every time upload happens

# uploading via Incremental Load
1. Merge Mode
sqoop import --connect jdbc:mysql://localhost:3306/hadooptrainingdb --username root --password cloudera --table employee --target-dir /MyHadoop/employeeincrement -m 1 --incremental lastmodified --check-column last_modified --last-value '2019-12-05 01:21:17' -merge-key emp_id

2. Append
# uploading via Incremental Load
sqoop import --connect jdbc:mysql://localhost:3306/hadooptrainingdb --username root --password cloudera --table employee --target-dir /MyHadoop/employeeappend -m 1 --incremental lastmodified --check-column last_modified --last-value '2019-12-05 01:21:17' --append

#### Sqoop Job #####  
- No need to send last update timestamp
- sqoob job know when the last job has run.
sqoop job --create emp-incremental-job-1 -- import --connect jdbc:mysql://localhost:3306/hadooptrainingdb --username root --password cloudera --table employee --target-dir /MyHadoop/employeeappendjob -m 1 --incremental lastmodified --check-column last_modified

# sqoop job --list

# Running a sqoop job
sqoop job --exec <job name>

