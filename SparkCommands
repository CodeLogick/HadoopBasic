Spark Shell
1. PySpark : For Python, Interactive shell to run python, all python cmds plus Spark APIs are here
2. Spark Shell - For Scala

########### PySpark ################

SparkContext (sc) is default available in pyspark
RDD - Resillient Distributed Dataset

####  Loading data via RDD

rdd1 = sc.textFile("/MyHadoop/employeeincrement/part-r-00000")
collection = rdd1.collect()
for line in collection:                                                     
    print(line)

Two types of operation can be performed on RDD
1. Transformation - Intermediate operations, Ex. Filter, peek, Map- Lazy function.
		    It creates RDD only when action is called.
2. Actions - Terminal operations, Ex. Count, 


Types of RDD:
1.Single Element RDD
2.Pair RDD- Each and every element of RDD is a pair -- Tuple (Hi, 4)

Lambda and Rdd Commands

##Map function
	rddUpperCase=rddUpper.map(lamba s : s.upper())
##Filter fuction
	rddFilteredWithI= rddUpperCase.filter(lambda s : s.startswith("I"))
##Saving the output in text file
	rddFilteredWithI.saveAsTextFile("/MyHadoop/Spark/")
## To get the output as collection
	rddFilteredWithI.collect()
## Converting Single RDD to pair
	rddTuple=sc.textFile("/MyHadoop/Spark/Users/users.txt").map(lambda s : s.split("\t").map(lambda line : (line[0],line[1])

## For Loop in Python
for filtered in rddFilteredWithI.collect() :
...     print(filtered)
... 

## Printing Tuple
>>> for pair in rddTuple.collect():
...     print(pair[0])




####(2,(3,4))
rddDoubleTuple = sc.textFile("/MyHadoop/Spark/Users/users1.txt").map(lambda s : s.split("\t")).map(lambda line : (line[0],(line[1],line[2])))
>>> rddCollect = rddDoubleTuple.collect()
>>> for line in rddCollect:
...     print(line)


##### Flat Map ######

for w in count.reduceByKey(lambda acc,value : acc + value).sortBy(lambda tup : tup[1], False).collect():
...     print(w)

The second param in sortBy (false) indicates ascending or desending, false means desc)


######### Spark Dataframe and SQL ###########

Dataframe contains name coulumns
 -collection with schema
 -can query with sql

while RDD is just a collection of objects 

rdd1 = sc.textFile("").map(lambda line : line.split(","))
rdd2 = rdd1.map(lambda arr : [arr[0],arr[1],arr[2],arr[3]])
df = rdd2.toDF("CustomerId","FirstName","LastName","Email") -- this take some time coz data is huge

# now with this df we can query like sql
df.registerTempTable("Customer")
df1 = sqlContext.sql("select * from customer")
df1.show(5)

# DataFrame we can save also as json
df1.write.format("json").save("path in hadoop")




## Creating a Dataframe from json format data in HDFS

{"id":"1","name":"Sanjay","designation":"Manager"}
{"id":"2","name":"Ajay","designation":"Manager1"}
{"id":"1","name":"Raj","designation":"Manager2"}
{"id":"1","name":"Vihjay","designation":"Manager3"}

empDf = sqlContext.read.format("json").load("/MyHadoop/Spark/JsonData.txt").select("id","name","designation")
empDf.registerTempTable("employee")
sqlContext.sql("select * from employee where designation = 'Manager'").show()


## Parquet format to DataFrame
df = sqlContext.read.format("parquet").load("/MyHadoop/employeeparquet/ebaf37fa-c08c-4563-9817-e42f21d52ad7.parquet").show()
df.registerTempTable("emp")
sqlContext.sql("select * from emp").show()

#### Joins with DataFrames ####

customerDf = sc.textFile("/MyHadoop/Spark/Customers").map(lambda line : line.split(",")).map(lambda arr : [arr[0],arr[1],arr[2]]).toDF(["CustomerId","FirstName","LastName"])

orderDf = sc.textFile("/MyHadoop/Spark/Orders").map(lambda line : line.split(",")).map(lambda arr : [arr[0],arr[1],arr[2],arr[3]]).toDF(["OrderId","OrderDate","CustomerId","Status"])

customerDf.registerTempTable("Customer")
orderDf.registerTempTable("Order")

joinDf = sqlContext.sql("select a.*, b.* from Customer a, Order b where a.CustomerId = b.CustomerId")

countDf = sqlContext.sql("select a.CustomerId, count(*) from Customer a, Order b where a.CustomerId = b.CustomerId group by a.CustomerId")





##################### Spark Streaming #################################

Spark Streaming Divides the data stream into Batches of nl seconds



## submitting python code
spark-submit kafkastreamtest.py
